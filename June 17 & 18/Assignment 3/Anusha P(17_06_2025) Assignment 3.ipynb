{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0246809-078e-4014-8645-1233c306bc2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "937b0596-e758-4d6a-958e-a66db85097a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data Ingestion & Schema Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b23baaa-fd9e-4969-92d8-db7b73a721e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|\n+----------+-----+----------+-------+---------+----------+---------+------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote|\n+----------+-----+----------+-------+---------+----------+---------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV using inferred schema.\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"file:/Workspace/Users/azuser3548_mml.local@techademy.com/employee_timesheet.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ea836e9-f4ef-4c6c-be98-00cb0969be01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|\n+----------+-----+----------+-------+---------+----------+---------+------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote|\n+----------+-----+----------+-------+---------+----------+---------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Load the same file with schema explicitly defined.\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "    StructField(\"EmployeeID\", StringType()),\n",
    "    StructField(\"Name\", StringType()),\n",
    "    StructField(\"Department\", StringType()),\n",
    "    StructField(\"Project\", StringType()),\n",
    "    StructField(\"WorkHours\", IntegerType()),\n",
    "    StructField(\"WorkDate\", DateType()),\n",
    "    StructField(\"Location\", StringType()),\n",
    "    StructField(\"Mode\", StringType())\n",
    "])\n",
    "df_manual = spark.read.option(\"header\", True).schema(schema).csv(\"file:/Workspace/Users/azuser3548_mml.local@techademy.com/employee_timesheet.csv\")\n",
    "df_manual.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "643e9fc4-6440-4153-88ad-40b0fa02fa2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------+---------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|  Weekday|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|Wednesday|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|Wednesday|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote| Thursday|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|   Friday|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|   Friday|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote| Saturday|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Add a new column Weekday extracted from WorkDate .\n",
    "from pyspark.sql.functions import date_format\n",
    "df = df.withColumn(\"Weekday\", date_format(\"WorkDate\", \"EEEE\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96cbcdf3-1dbf-46a4-93d5-69d2a75c9720",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Aggregations & Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d38f368-dc17-44c2-84af-ba57e54f2f5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+\n|EmployeeID| Name|TotalHours|\n+----------+-----+----------+\n|      E103| John|         5|\n|      E104|Meena|         6|\n|      E102|  Raj|        15|\n|      E101|Anita|        17|\n+----------+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Calculate total work hours by employee.\n",
    "from pyspark.sql.functions import sum\n",
    "df.groupBy(\"EmployeeID\", \"Name\").agg(sum(\"WorkHours\").alias(\"TotalHours\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dca23285-bc73-4749-be05-c0b1a7f3b9dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n|Department|         AvgHours|\n+----------+-----------------+\n|        HR|              7.5|\n|   Finance|              5.0|\n|        IT|7.666666666666667|\n+----------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Calculate average work hours per department.\n",
    "from pyspark.sql.functions import avg\n",
    "df.groupBy(\"Department\").agg(avg(\"WorkHours\").alias(\"AvgHours\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecf04662-6c57-4e6e-8453-388e91c8e3cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+----+\n|EmployeeID| Name|TotalHours|Rank|\n+----------+-----+----------+----+\n|      E101|Anita|        17|   1|\n|      E102|  Raj|        15|   2|\n+----------+-----+----------+----+\n\n"
     ]
    }
   ],
   "source": [
    "# Get top 2 employees by total hours using window function.\n",
    "from pyspark.sql.functions import desc, rank\n",
    "from pyspark.sql.window import Window\n",
    "window_spec = Window.orderBy(desc(\"TotalHours\"))\n",
    "top_hours = df.groupBy(\"EmployeeID\", \"Name\").agg(sum(\"WorkHours\").alias(\"TotalHours\"))\n",
    "top_hours.withColumn(\"Rank\", rank().over(window_spec)).filter(\"Rank <= 2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33457b36-4437-416e-a6d8-eadcda173ba5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Date Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f863afb4-a3f3-4083-bfd3-7e0087083ca1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+-------+---------+----------+--------+------+--------+\n|EmployeeID|Name|Department|Project|WorkHours|  WorkDate|Location|  Mode| Weekday|\n+----------+----+----------+-------+---------+----------+--------+------+--------+\n|      E102| Raj|        HR|   Beta|        8|2024-05-04|  Mumbai|Remote|Saturday|\n+----------+----+----------+-------+---------+----------+--------+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Filter entries where WorkDate falls on a weekend.\n",
    "from pyspark.sql.functions import dayofweek\n",
    "df.filter(dayofweek(\"WorkDate\").isin([1, 7])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afbd99a7-929f-4973-b62f-c3274f2899e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------+---------+------------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|  Weekday|RunningTotal|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+------------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|Wednesday|           8|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|   Friday|          17|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|Wednesday|           7|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote| Saturday|          15|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote| Thursday|           5|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|   Friday|           6|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Calculate running total of hours per employee using window.\n",
    "window_spec2 = Window.partitionBy(\"EmployeeID\").orderBy(\"WorkDate\")\n",
    "df.withColumn(\"RunningTotal\", sum(\"WorkHours\").over(window_spec2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2e40d1e-3d35-44ce-b11d-5faafdfa696f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Joining DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf484805-f5a4-427b-a653-37ad1007c74e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+--------+\n|EmployeeID| Name|Department|DeptHead|\n+----------+-----+----------+--------+\n|      E101|Anita|        IT|   Anand|\n|      E102|  Raj|        HR|  Shruti|\n|      E103| John|   Finance|   Kamal|\n|      E101|Anita|        IT|   Anand|\n|      E104|Meena|        IT|   Anand|\n|      E102|  Raj|        HR|  Shruti|\n+----------+-----+----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Create department_location.csv :\n",
    "# Department,DeptHead\n",
    "# IT,Anand\n",
    "# HR,Shruti\n",
    "# Finance,Kamal\n",
    "dept_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"file:/Workspace/Users/azuser3548_mml.local@techademy.com/department_location.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9cd95ea-2083-4b17-9ed3-a947d04242f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+--------+\n|EmployeeID| Name|Department|DeptHead|\n+----------+-----+----------+--------+\n|      E101|Anita|        IT|   Anand|\n|      E102|  Raj|        HR|  Shruti|\n|      E103| John|   Finance|   Kamal|\n|      E101|Anita|        IT|   Anand|\n|      E104|Meena|        IT|   Anand|\n|      E102|  Raj|        HR|  Shruti|\n+----------+-----+----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Join with timesheet data and list all employees with their DeptHead.\n",
    "df_joined = df.join(dept_df, \"Department\", \"left\")\n",
    "df_joined.select(\"EmployeeID\", \"Name\", \"Department\", \"DeptHead\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "260ac6bd-4ed4-4766-9ea3-32ad0a52e2c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Pivot & Unpivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "302ef1d5-1a2e-4222-a34a-e5e0664b15f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+-----+\n|EmployeeID|Alpha|Beta|Gamma|\n+----------+-----+----+-----+\n|      E103|    5|NULL| NULL|\n|      E104| NULL|NULL|    6|\n|      E101|   17|NULL| NULL|\n|      E102| NULL|  15| NULL|\n+----------+-----+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Pivot table: total hours per employee per project.\n",
    "df.groupBy(\"EmployeeID\").pivot(\"Project\").agg(sum(\"WorkHours\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1053656-6866-43bc-81b2-4561125c75aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+-----+\n|EmployeeID| Name|  Mode|Hours|\n+----------+-----+------+-----+\n|      E101|Anita|Remote|    8|\n|      E102|  Raj|Onsite|    7|\n|      E103| John|Remote|    5|\n|      E101|Anita|Remote|    9|\n|      E104|Meena|Onsite|    6|\n|      E102|  Raj|Remote|    8|\n+----------+-----+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Unpivot example: Convert mode-specific hours into rows.\n",
    "from pyspark.sql.functions import expr\n",
    "unpivot = \"stack(1, Mode, WorkHours) as (Mode, Hours)\"\n",
    "df.select(\"EmployeeID\", \"Name\", expr(unpivot)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2691193b-270b-4763-87ec-c26ab0ad60b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "UDF & Conditional Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "153a8e37-eb00-4977-8913-ee9832147087",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------+---------+----------------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|  Weekday|WorkloadCategory|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+----------------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|Wednesday|            Full|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|Wednesday|         Partial|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote| Thursday|         Partial|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|   Friday|            Full|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|   Friday|         Partial|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote| Saturday|            Full|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Create a UDF to classify work hours:\n",
    "# def workload_tag(hours):\n",
    "# if hours >= 8: return \"Full\"\n",
    "# elif hours >= 4: return \"Partial\"\n",
    "# else: return \"Light\"\n",
    "# Add a column WorkloadCategory using this UDF.\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "def workload_tag(hours):\n",
    "    if hours >= 8:\n",
    "        return \"Full\"\n",
    "    elif hours >= 4:\n",
    "        return \"Partial\"\n",
    "    else:\n",
    "        return \"Light\"\n",
    "workload_udf = udf(workload_tag, StringType())\n",
    "df = df.withColumn(\"WorkloadCategory\", workload_udf(\"WorkHours\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62a162fd-f18b-4590-b90a-5ced6e7eac58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Nulls and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fa27da1-8ef2-43f4-9b9c-95daf9c143a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------+---------+----------------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|  Weekday|WorkloadCategory|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+----------------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|Wednesday|            Full|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|  NULL|Wednesday|         Partial|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote| Thursday|         Partial|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|   Friday|            Full|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|   Friday|         Partial|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|  NULL| Saturday|            Full|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Introduce some nulls in Mode column.\n",
    "from pyspark.sql.functions import when, col\n",
    "df_null = df.withColumn(\"Mode\", when(col(\"EmployeeID\") == \"E102\", None).otherwise(col(\"Mode\")))\n",
    "df_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d305c669-b85d-495b-a32c-529b71762aaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------------+---------+----------------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|        Mode|  Weekday|WorkloadCategory|\n+----------+-----+----------+-------+---------+----------+---------+------------+---------+----------------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|      Remote|Wednesday|            Full|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Not Provided|Wednesday|         Partial|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|      Remote| Thursday|         Partial|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|      Remote|   Friday|            Full|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|      Onsite|   Friday|         Partial|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Not Provided| Saturday|            Full|\n+----------+-----+----------+-------+---------+----------+---------+------------+---------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Fill nulls with \"Not Provided\".\n",
    "df_filled = df_null.fillna({\"Mode\": \"Not Provided\"})\n",
    "df_filled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "926f1ad2-a683-47ab-bc1e-5b7475500d0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------------+---------+----------------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|        Mode|  Weekday|WorkloadCategory|\n+----------+-----+----------+-------+---------+----------+---------+------------+---------+----------------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|      Remote|Wednesday|            Full|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Not Provided|Wednesday|         Partial|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|      Remote| Thursday|         Partial|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|      Remote|   Friday|            Full|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|      Onsite|   Friday|         Partial|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Not Provided| Saturday|            Full|\n+----------+-----+----------+-------+---------+----------+---------+------------+---------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where WorkHours < 4.\n",
    "cleaned_df = df_filled.filter(col(\"WorkHours\") >= 4)\n",
    "cleaned_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5baa624-81be-4074-a170-37850ac9bc74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Advanced Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eae4dfc-8bcc-45b6-aa86-a02bb530a8d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------+\n|EmployeeID|remote_ratio|   WorkerType|\n+----------+------------+-------------+\n|      E103|         1.0|Remote Worker|\n|      E104|         0.0| Onsite/Mixed|\n|      E101|         1.0|Remote Worker|\n|      E102|         0.5| Onsite/Mixed|\n+----------+------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Use when-otherwise to mark employees as \"Remote Worker\" if >80% entries are Remote.\n",
    "from pyspark.sql.functions import sum, count, when\n",
    "remote = df.groupBy(\"EmployeeID\").agg(\n",
    "    (sum(when(col(\"Mode\") == \"Remote\", 1).otherwise(0)) / count(\"Mode\")).alias(\"remote_ratio\"))\n",
    "remote_flag = remote.withColumn(\"WorkerType\", when(col(\"remote_ratio\") > 0.8, \"Remote Worker\").otherwise(\"Onsite/Mixed\"))\n",
    "remote_flag.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "029f542c-6fb1-428c-9e90-b2528419be03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------+---------+----------------+----------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|  Weekday|WorkloadCategory|ExtraHours|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+----------------+----------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|Wednesday|            Full|         0|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|Wednesday|         Partial|         0|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote| Thursday|         Partial|         0|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|   Friday|            Full|         1|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|   Friday|         Partial|         0|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote| Saturday|            Full|         0|\n+----------+-----+----------+-------+---------+----------+---------+------+---------+----------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Add a new column ExtraHours where hours > 8.\n",
    "df = df.withColumn(\"ExtraHours\", when(col(\"WorkHours\") > 8, col(\"WorkHours\") - 8).otherwise(0))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82c4093d-ecff-40d6-9c8e-8fcbc50706cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Union + Duplicate Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f557e247-0ddb-41de-9ebf-d658ce9d9e44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+-------+---------+----------+---------+------+---------+----------------+----------+\n|EmployeeID|  Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|  Weekday|WorkloadCategory|ExtraHours|\n+----------+------+----------+-------+---------+----------+---------+------+---------+----------------+----------+\n|      E101| Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|Wednesday|            Full|         0|\n|      E102|   Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|Wednesday|         Partial|         0|\n|      E103|  John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote| Thursday|         Partial|         0|\n|      E101| Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|   Friday|            Full|         1|\n|      E104| Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|   Friday|         Partial|         0|\n|      E102|   Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote| Saturday|            Full|         0|\n|      E999|Intern|        IT|Onboard|        6|2024-05-05|Bangalore|Remote|   Sunday|         Partial|         0|\n+----------+------+----------+-------+---------+----------+---------+------+---------+----------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Append a dummy timesheet for new interns using unionByName() .\n",
    "dummy_df = spark.createDataFrame([\n",
    "    (\"E999\", \"Intern\", \"IT\", \"Onboard\", 6, \"2024-05-05\", \"Bangalore\", \"Remote\", \"Sunday\", \"Partial\", 0)\n",
    "], df.columns)\n",
    "combined_df = df.unionByName(dummy_df)\n",
    "combined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "652873e7-527d-41f5-b6bd-f3216f72610a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+-------+---------+----------+---------+------+---------+----------------+----------+\n|EmployeeID|  Name|Department|Project|WorkHours|  WorkDate| Location|  Mode|  Weekday|WorkloadCategory|ExtraHours|\n+----------+------+----------+-------+---------+----------+---------+------+---------+----------------+----------+\n|      E101| Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote|Wednesday|            Full|         0|\n|      E104| Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite|   Friday|         Partial|         0|\n|      E103|  John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote| Thursday|         Partial|         0|\n|      E101| Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote|   Friday|            Full|         1|\n|      E102|   Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|Remote| Saturday|            Full|         0|\n|      E102|   Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite|Wednesday|         Partial|         0|\n|      E999|Intern|        IT|Onboard|        6|2024-05-05|Bangalore|Remote|   Sunday|         Partial|         0|\n+----------+------+----------+-------+---------+----------+---------+------+---------+----------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate rows based on all columns.\n",
    "final_df = combined_df.dropDuplicates()\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e925b56c-22ff-4871-9129-53f9d02bf662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "668f16de-d58a-48e9-8efe-aeb608b40beb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Anusha P(17_06_2025) Assignment 3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}