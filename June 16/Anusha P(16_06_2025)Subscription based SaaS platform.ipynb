{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31dfdd5b-444d-4267-b09c-76125282d1fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8ce2453-b309-49d9-ad07-68837af04652",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Subscription Engagement Score (Real Metric Modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1a02bf4-9778-48ac-b267-fdd05c92faca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+--------+----------+----------+--------+--------+---------+\n|SubscriptionID|UserID|PlanType| StartDate|   EndDate|PriceUSD|IsActive|AutoRenew|\n+--------------+------+--------+----------+----------+--------+--------+---------+\n|        SUB001|  U001|   Basic|2024-01-01|2024-04-01|    30.0|    true|     true|\n|        SUB002|  U002|     Pro|2024-02-15|2024-05-15|    90.0|    true|    false|\n|        SUB003|  U003|     Pro|2024-03-10|2024-06-10|    90.0|   false|    false|\n|        SUB004|  U001| Premium|2024-04-05|2024-07-05|   120.0|    true|     true|\n|        SUB005|  U004|   Basic|2024-01-20|2024-04-20|    30.0|   false|    false|\n+--------------+------+--------+----------+----------+--------+--------+---------+\n\n+------+-------------------+---------+-----------+\n|UserID|          EventTime|EventType|FeatureUsed|\n+------+-------------------+---------+-----------+\n|  U001|2024-04-07 10:22:00|    login|  Dashboard|\n|  U002|2024-04-08 11:10:00|   upload|    Reports|\n|  U003|2024-04-09 09:45:00| download|  Analytics|\n|  U001|2024-04-10 16:00:00|   logout|  Dashboard|\n|  U004|2024-04-11 12:00:00|    login|  Dashboard|\n+------+-------------------+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "sub_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"dbfs:/FileStore/shared_uploads/azuser3548_mml.local@techademy.com/subscriptions.csv\")\n",
    "activity_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"dbfs:/FileStore/shared_uploads/azuser3548_mml.local@techademy.com/user_activity.csv\")\n",
    "sub_df.show()\n",
    "activity_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "366eb566-4c16-4d7e-9baf-4727608a857c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+--------+----------+----------+--------+--------+---------+-----------+---------------+------------------+\n|UserID|SubscriptionID|PlanType| StartDate|   EndDate|PriceUSD|IsActive|AutoRenew|active_days|events_per_user|  engagement_score|\n+------+--------------+--------+----------+----------+--------+--------+---------+-----------+---------------+------------------+\n|  U001|        SUB001|   Basic|2024-01-01|2024-04-01|    30.0|    true|     true|         91|              2|0.6593406593406594|\n|  U002|        SUB002|     Pro|2024-02-15|2024-05-15|    90.0|    true|    false|         90|              1|               1.0|\n|  U003|        SUB003|     Pro|2024-03-10|2024-06-10|    90.0|   false|    false|         92|              1|0.9782608695652174|\n|  U001|        SUB004| Premium|2024-04-05|2024-07-05|   120.0|    true|     true|         91|              2|2.6373626373626378|\n|  U004|        SUB005|   Basic|2024-01-20|2024-04-20|    30.0|   false|    false|         91|              1|0.3296703296703297|\n+------+--------------+--------+----------+----------+--------+--------+---------+-----------+---------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Combine both datasets.\n",
    "# Calculate:\n",
    "# active_days = EndDate - StartDate\n",
    "# events_per_user = count(EventType) grouped by UserID\n",
    "# Create a score: engagement_score = (events_per_user / active_days) * PriceUSD\n",
    "\n",
    "from pyspark.sql.functions import datediff, count, col\n",
    "sub_df = sub_df.withColumn(\"active_days\", datediff(\"EndDate\", \"StartDate\"))\n",
    "events_df = activity_df.groupBy(\"UserID\").agg(count(\"*\").alias(\"events_per_user\"))\n",
    "engagement_df = sub_df.join(events_df, \"UserID\", \"left\") \\\n",
    "    .withColumn(\"engagement_score\", (col(\"events_per_user\") / col(\"active_days\")) * col(\"PriceUSD\"))\n",
    "engagement_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60662fd1-09d6-4a28-a3dd-1e97bb3e2045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Anomaly Detection via SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5379746d-8892-4c53-9d19-e8e41f1dbdd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+--------+-------------------+\n|SubscriptionID|UserID|IsActive|      last_activity|\n+--------------+------+--------+-------------------+\n|        SUB003|  U003|   false|2024-04-09 09:45:00|\n|        SUB005|  U004|   false|2024-04-11 12:00:00|\n+--------------+------+--------+-------------------+\n\n+--------------+------+---------+-------------------+\n|SubscriptionID|UserID|AutoRenew|      last_activity|\n+--------------+------+---------+-------------------+\n|        SUB001|  U001|     true|2024-04-10 16:00:00|\n|        SUB004|  U001|     true|2024-04-10 16:00:00|\n+--------------+------+---------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Identify users with:\n",
    "# Subscription inactive but recent activity\n",
    "# AutoRenew is true but no events in 30 days\n",
    "# Use SQL views to expose this logic.\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp, to_date\n",
    "sub_df.createOrReplaceTempView(\"subscriptions\")\n",
    "activity_df = activity_df.withColumn(\"EventTime\", to_timestamp(\"EventTime\"))\n",
    "activity_df.createOrReplaceTempView(\"user_activity\")\n",
    "\n",
    "spark.sql(\"\"\"create or replace temp view inactive_with_recent_activity AS\n",
    "SELECT s.SubscriptionID,s.UserID,s.IsActive,max(a.EventTime) AS last_activity\n",
    "FROM subscriptions s\n",
    "JOIN user_activity a ON s.UserID = a.UserID\n",
    "WHERE s.IsActive = false\n",
    "GROUP BY s.SubscriptionID, s.UserID, s.IsActive\n",
    "HAVING datediff(to_date('2024-04-15'), last_activity)<30\"\"\")\n",
    "\n",
    "spark.sql(\"select * from inactive_with_recent_activity\").show()\n",
    "\n",
    "spark.sql(\"\"\"CREATE OR REPLACE TEMP VIEW autorenew_no_recent_activity AS\n",
    "SELECT s.SubscriptionID,s.UserID,s.AutoRenew,MAX(a.EventTime) AS last_activity\n",
    "FROM subscriptions s\n",
    "LEFT JOIN user_activity a ON s.UserID = a.UserID\n",
    "WHERE s.AutoRenew = true\n",
    "GROUP BY s.SubscriptionID, s.UserID, s.AutoRenew\n",
    "HAVING last_activity IS NULL OR datediff(current_date(), last_activity) > 30\"\"\")\n",
    "\n",
    "spark.sql(\"select * from autorenew_no_recent_activity\").show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4aee7057-ef59-4b87-b408-751270ff487f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Delta Lake + Merge Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "046805d8-65d4-4408-a582-c977ab5d9400",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>SubscriptionID</th><th>UserID</th><th>PlanType</th><th>StartDate</th><th>EndDate</th><th>PriceUSD</th><th>IsActive</th><th>AutoRenew</th><th>active_days</th></tr></thead><tbody><tr><td>SUB001</td><td>U001</td><td>Basic</td><td>2024-01-01</td><td>2024-04-01</td><td>30.0</td><td>true</td><td>true</td><td>91</td></tr><tr><td>SUB002</td><td>U002</td><td>Pro</td><td>2024-02-15</td><td>2024-05-15</td><td>90.0</td><td>true</td><td>false</td><td>90</td></tr><tr><td>SUB004</td><td>U001</td><td>Premium</td><td>2024-04-05</td><td>2024-07-05</td><td>120.0</td><td>true</td><td>true</td><td>91</td></tr><tr><td>SUB005</td><td>U004</td><td>Basic</td><td>2024-01-20</td><td>2024-04-20</td><td>30.0</td><td>false</td><td>false</td><td>91</td></tr><tr><td>SUB003</td><td>U003</td><td>Pro</td><td>2024-03-10</td><td>2024-06-10</td><td>95.0</td><td>false</td><td>false</td><td>92</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "SUB001",
         "U001",
         "Basic",
         "2024-01-01",
         "2024-04-01",
         30.0,
         true,
         true,
         91
        ],
        [
         "SUB002",
         "U002",
         "Pro",
         "2024-02-15",
         "2024-05-15",
         90.0,
         true,
         false,
         90
        ],
        [
         "SUB004",
         "U001",
         "Premium",
         "2024-04-05",
         "2024-07-05",
         120.0,
         true,
         true,
         91
        ],
        [
         "SUB005",
         "U004",
         "Basic",
         "2024-01-20",
         "2024-04-20",
         30.0,
         false,
         false,
         91
        ],
        [
         "SUB003",
         "U003",
         "Pro",
         "2024-03-10",
         "2024-06-10",
         95.0,
         false,
         false,
         92
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "SubscriptionID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "UserID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "PlanType",
         "type": "\"string\""
        },
        {
         "metadata": "{\"__detected_date_formats\":\"yyyy-M-d\"}",
         "name": "StartDate",
         "type": "\"date\""
        },
        {
         "metadata": "{\"__detected_date_formats\":\"yyyy-M-d\"}",
         "name": "EndDate",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "PriceUSD",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "IsActive",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "AutoRenew",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "active_days",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Imagine a billing fix needs to be applied:\n",
    "# For all Pro plans in March, increase price by $5 retroactively.\n",
    "# Use MERGE INTO on Delta table to apply the change.\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "sub_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/subscriptions\")\n",
    "delta_subs = DeltaTable.forPath(spark, \"/delta/subscriptions\")\n",
    "update_df = sub_df.filter((col(\"PlanType\") == \"Pro\") & (col(\"StartDate\").startswith(\"2024-03\"))).withColumn(\"PriceUSD\", col(\"PriceUSD\") + 5)\n",
    "delta_subs.alias(\"target\").merge(update_df.alias(\"updates\"),\"target.SubscriptionID = updates.SubscriptionID\").whenMatchedUpdate(set={\"PriceUSD\": \"updates.PriceUSD\"}).execute()\n",
    "display(spark.read.format(\"delta\").load(\"/delta/subscriptions\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af5d291d-cfa5-4191-ad44-2d70392acecb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Time Travel Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00036450-da2e-4dc6-bc45-c07677a5f8af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before the change:\n+--------------+------+--------+----------+----------+--------+--------+---------+-----------+\n|SubscriptionID|UserID|PlanType| StartDate|   EndDate|PriceUSD|IsActive|AutoRenew|active_days|\n+--------------+------+--------+----------+----------+--------+--------+---------+-----------+\n|        SUB001|  U001|   Basic|2024-01-01|2024-04-01|    30.0|    true|     true|         91|\n|        SUB002|  U002|     Pro|2024-02-15|2024-05-15|    90.0|    true|    false|         90|\n|        SUB003|  U003|     Pro|2024-03-10|2024-06-10|    90.0|   false|    false|         92|\n|        SUB004|  U001| Premium|2024-04-05|2024-07-05|   120.0|    true|     true|         91|\n|        SUB005|  U004|   Basic|2024-01-20|2024-04-20|    30.0|   false|    false|         91|\n+--------------+------+--------+----------+----------+--------+--------+---------+-----------+\n\n+-------+-------------------+----------------+--------------------+---------+--------------------+----+-----------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|version|          timestamp|          userId|            userName|operation| operationParameters| job|         notebook|           clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+-------------------+----------------+--------------------+---------+--------------------+----+-----------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|      6|2025-06-18 06:57:52|4042796083082360|azuser3548_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{367143923686644}|0612-043650-nhuexwr6|          5|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|      5|2025-06-18 06:57:50|4042796083082360|azuser3548_mml.lo...|    MERGE|{predicate -> [\"(...|NULL|{367143923686644}|0612-043650-nhuexwr6|          4|WriteSerializable|        false|{numTargetRowsCop...|        NULL|Databricks-Runtim...|\n|      4|2025-06-18 06:57:47|4042796083082360|azuser3548_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{367143923686644}|0612-043650-nhuexwr6|          3|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n|      3|2025-06-18 06:56:16|4042796083082360|azuser3548_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{367143923686644}|0612-043650-nhuexwr6|          2|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|      2|2025-06-18 06:56:13|4042796083082360|azuser3548_mml.lo...|    MERGE|{predicate -> [\"(...|NULL|{367143923686644}|0612-043650-nhuexwr6|          1|WriteSerializable|        false|{numTargetRowsCop...|        NULL|Databricks-Runtim...|\n|      1|2025-06-18 06:56:07|4042796083082360|azuser3548_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{367143923686644}|0612-043650-nhuexwr6|          0|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n|      0|2025-06-18 06:55:31|4042796083082360|azuser3548_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{367143923686644}|0612-043650-nhuexwr6|       NULL|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n+-------+-------------------+----------------+--------------------+---------+--------------------+----+-----------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n\nAfter the change:\n+--------------+------+--------+----------+----------+--------+--------+---------+-----------+\n|SubscriptionID|UserID|PlanType| StartDate|   EndDate|PriceUSD|IsActive|AutoRenew|active_days|\n+--------------+------+--------+----------+----------+--------+--------+---------+-----------+\n|        SUB001|  U001|   Basic|2024-01-01|2024-04-01|    30.0|    true|     true|         91|\n|        SUB002|  U002|     Pro|2024-02-15|2024-05-15|    90.0|    true|    false|         90|\n|        SUB004|  U001| Premium|2024-04-05|2024-07-05|   120.0|    true|     true|         91|\n|        SUB005|  U004|   Basic|2024-01-20|2024-04-20|    30.0|   false|    false|         91|\n|        SUB003|  U003|     Pro|2024-03-10|2024-06-10|    95.0|   false|    false|         92|\n+--------------+------+--------+----------+----------+--------+--------+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Show describe history of the table before and after the billing fix.\n",
    "# Query using VERSION AS OF to prove the issue existed.\n",
    "\n",
    "delta_subs.history().show()\n",
    "\n",
    "# Read previous version\n",
    "print(\"Before the change:\")\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/delta/subscriptions\").show()\n",
    "\n",
    "print(\"After the change:\")\n",
    "spark.read.format(\"delta\").load(\"/delta/subscriptions\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8ea0d5f-8b95-4948-ad05-01435f72b95a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Build Tier Migration Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cae3ed7a-dd63-43e0-9278-142d1f7c436e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+--------+---------+-------+--------+--------+---------+-----------+-------------+\n|SubscriptionID|UserID|PlanType|StartDate|EndDate|PriceUSD|IsActive|AutoRenew|active_days|previous_plan|\n+--------------+------+--------+---------+-------+--------+--------+---------+-----------+-------------+\n+--------------+------+--------+---------+-------+--------+--------+---------+-----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Identify users who upgraded:\n",
    "# From Basic → Pro → Premium\n",
    "# Use PySpark with lag() function to model this.\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag\n",
    "\n",
    "window_spec = Window.partitionBy(\"UserID\").orderBy(\"StartDate\")\n",
    "\n",
    "tier_df = sub_df.withColumn(\"previous_plan\", lag(\"PlanType\").over(window_spec)) \\\n",
    "    .filter((col(\"previous_plan\") == \"Basic\") & (col(\"PlanType\") == \"Pro\") |\n",
    "            (col(\"previous_plan\") == \"Pro\") & (col(\"PlanType\") == \"Premium\"))\n",
    "tier_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74132b17-e6a3-43cb-8399-37607aa0b5d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Power Users Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "148b6477-e9dc-4517-8023-67bc13081289",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>UserID</th><th>features_used</th><th>login_count</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "UserID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "features_used",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "login_count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a power user as:\n",
    "# Used ≥ 2 features\n",
    "# Logged in ≥ 3 times\n",
    "# Create a separate Delta table power_users\n",
    "\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "power_users_df = activity_df.groupBy(\"UserID\").agg(\n",
    "    countDistinct(\"FeatureUsed\").alias(\"features_used\"),\n",
    "    count(\"*\").alias(\"login_count\")\n",
    ").filter((col(\"features_used\") >= 2) & (col(\"login_count\") >= 3))\n",
    "power_users_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"power_users\")\n",
    "display(power_users_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d900450-8c5e-455f-b85c-9e48923c479d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Session Replay View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff767e91-79cd-4e2d-b8af-b3e1271a11c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------------------+------------+\n|UserID|prev_event|          EventTime|session_secs|\n+------+----------+-------------------+------------+\n|  U001|     login|2024-04-10 16:00:00|      279480|\n+------+----------+-------------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Build a user session trace table using:\n",
    "# Window.partitionBy(\"UserID\").orderBy(\"EventTime\")\n",
    "# Show how long each user spent between login and logout events.\n",
    "\n",
    "from pyspark.sql.functions import lead, unix_timestamp,when\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window = Window.partitionBy(\"UserID\").orderBy(\"EventTime\")\n",
    "\n",
    "session_replay = activity_df.withColumn(\"event_ts\", unix_timestamp(\"EventTime\")) \\\n",
    "    .withColumn(\"prev_ts\", lag(\"event_ts\").over(window)) \\\n",
    "    .withColumn(\"prev_event\", lag(\"EventType\").over(window)) \\\n",
    "    .withColumn(\"session_secs\", \n",
    "                when(col(\"EventType\") == \"logout\", col(\"event_ts\") - col(\"prev_ts\"))) \\\n",
    "    .filter(col(\"EventType\") == \"logout\")\n",
    "session_replay.select(\"UserID\", \"prev_event\", \"EventTime\", \"session_secs\").show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Anusha P(16_06_2025)Subscription based SaaS platform",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}