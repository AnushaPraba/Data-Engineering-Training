{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "PySpark Exercises â€“ Set 2 (Advanced)"
      ],
      "metadata": {
        "id": "88muRz8bkWNR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4UlX8Y8Tf9lS"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"AdvancedEmployeeTasks\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"Ananya\", \"HR\", 52000),\n",
        "    (\"Rahul\", \"Engineering\", 65000),\n",
        "    (\"Priya\", \"Engineering\", 60000),\n",
        "    (\"Zoya\", \"Marketing\", 48000),\n",
        "    (\"Karan\", \"HR\", 53000),\n",
        "    (\"Naveen\", \"Engineering\", 70000),\n",
        "    (\"Fatima\", \"Marketing\", 45000)\n",
        "]\n",
        "columns = [\"Name\", \"Department\", \"Salary\"]\n",
        "df_emp = spark.createDataFrame(data, columns)\n",
        "performance = [\n",
        "    (\"Ananya\", 2023, 4.5),\n",
        "    (\"Rahul\", 2023, 4.9),\n",
        "    (\"Priya\", 2023, 4.3),\n",
        "    (\"Zoya\", 2023, 3.8),\n",
        "    (\"Karan\", 2023, 4.1),\n",
        "    (\"Naveen\", 2023, 4.7),\n",
        "    (\"Fatima\", 2023, 3.9)\n",
        "]\n",
        "columns_perf = [\"Name\", \"Year\", \"Rating\"]\n",
        "df_perf = spark.createDataFrame(performance, columns_perf)"
      ],
      "metadata": {
        "id": "HHbmGkQJgLbY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GroupBy and Aggregations"
      ],
      "metadata": {
        "id": "_W51WQ_pkbzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Get the average salary by department\n",
        "from pyspark.sql.functions import avg\n",
        "df_emp.groupBy(\"Department\").agg(avg(\"Salary\").alias(\"AvgSalary\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7HJgsMIgQGq",
        "outputId": "4ac65a1f-b770-414b-e44c-b34e5ad0858a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+\n",
            "| Department|AvgSalary|\n",
            "+-----------+---------+\n",
            "|Engineering|  65000.0|\n",
            "|         HR|  52500.0|\n",
            "|  Marketing|  46500.0|\n",
            "+-----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  2. Count of employees per department\n",
        "df_emp.groupBy(\"Department\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtZtV2Lxg5SI",
        "outputId": "f292a43c-570c-4568-d526-035db4ed6af5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----+\n",
            "| Department|count|\n",
            "+-----------+-----+\n",
            "|Engineering|    3|\n",
            "|         HR|    2|\n",
            "|  Marketing|    2|\n",
            "+-----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Max and Min salary in Engineering\n",
        "from pyspark.sql.functions import max, min\n",
        "df_emp.filter(df_emp.Department == \"Engineering\").select(max(\"Salary\").alias(\"MaxSalary\"), min(\"Salary\").alias(\"MinSalary\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nKfgsDeh7t3",
        "outputId": "f0d07749-a0f3-4169-d032-dd7a7a31d643"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+\n",
            "|MaxSalary|MinSalary|\n",
            "+---------+---------+\n",
            "|    70000|    60000|\n",
            "+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Join and Combine Data"
      ],
      "metadata": {
        "id": "3Ntml1bskgBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  4. Inner join on Name\n",
        "df_joined = df_emp.join(df_perf, on=\"Name\", how=\"inner\")\n",
        "df_joined.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1D9bIgI2iJnq",
        "outputId": "6d4f518e-a7ca-4450-9ca6-5b6496607bfd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+------+----+------+\n",
            "|  Name| Department|Salary|Year|Rating|\n",
            "+------+-----------+------+----+------+\n",
            "|Ananya|         HR| 52000|2023|   4.5|\n",
            "|Fatima|  Marketing| 45000|2023|   3.9|\n",
            "| Karan|         HR| 53000|2023|   4.1|\n",
            "|Naveen|Engineering| 70000|2023|   4.7|\n",
            "| Priya|Engineering| 60000|2023|   4.3|\n",
            "| Rahul|Engineering| 65000|2023|   4.9|\n",
            "|  Zoya|  Marketing| 48000|2023|   3.8|\n",
            "+------+-----------+------+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Show each employee's salary and rating\n",
        "df_joined.select(\"Name\", \"Salary\", \"Rating\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQO3w6tZiQ-1",
        "outputId": "b09b7d48-7040-4704-c735-8a5a844ba405"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+------+\n",
            "|  Name|Salary|Rating|\n",
            "+------+------+------+\n",
            "|Ananya| 52000|   4.5|\n",
            "|Fatima| 45000|   3.9|\n",
            "| Karan| 53000|   4.1|\n",
            "|Naveen| 70000|   4.7|\n",
            "| Priya| 60000|   4.3|\n",
            "| Rahul| 65000|   4.9|\n",
            "|  Zoya| 48000|   3.8|\n",
            "+------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Filter employees with rating > 4.5 and salary > 60000.\n",
        "from pyspark.sql.functions import col\n",
        "df_joined.filter((col('Rating') > 4.5) & (col('Salary') > 60000)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdfYK83BiWJV",
        "outputId": "71833556-8209-4892-c397-177ed0d1f87e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+------+----+------+\n",
            "|  Name| Department|Salary|Year|Rating|\n",
            "+------+-----------+------+----+------+\n",
            "|Naveen|Engineering| 70000|2023|   4.7|\n",
            "| Rahul|Engineering| 65000|2023|   4.9|\n",
            "+------+-----------+------+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Window & Rank (Bonus Challenge)"
      ],
      "metadata": {
        "id": "YR8kQXvIkh4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Rank employees by salary department-wise.\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number\n",
        "window_spec = Window.partitionBy(\"Department\").orderBy(df_emp[\"Salary\"].desc())\n",
        "df_emp.withColumn(\"Rank\", row_number().over(window_spec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avRSthmQieRR",
        "outputId": "20685efa-583b-4d79-a1c3-b67f658676b3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+------+----+\n",
            "|  Name| Department|Salary|Rank|\n",
            "+------+-----------+------+----+\n",
            "|Naveen|Engineering| 70000|   1|\n",
            "| Rahul|Engineering| 65000|   2|\n",
            "| Priya|Engineering| 60000|   3|\n",
            "| Karan|         HR| 53000|   1|\n",
            "|Ananya|         HR| 52000|   2|\n",
            "|  Zoya|  Marketing| 48000|   1|\n",
            "|Fatima|  Marketing| 45000|   2|\n",
            "+------+-----------+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Cumulative salary in each department\n",
        "from pyspark.sql.functions import sum\n",
        "windowCum = Window.partitionBy(\"Department\").orderBy(\"Salary\").rowsBetween(Window.unboundedPreceding, 0)\n",
        "df_cumsum = df_joined.withColumn(\"CumulativeSalary\", sum(\"Salary\").over(windowCum))\n",
        "df_cumsum.select(\"Name\", \"Department\", \"Salary\", \"CumulativeSalary\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2-EE6rWjBwJ",
        "outputId": "c9ff7008-5a32-45bd-d1dc-a07c522d7bc1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+------+----------------+\n",
            "|  Name| Department|Salary|CumulativeSalary|\n",
            "+------+-----------+------+----------------+\n",
            "| Priya|Engineering| 60000|           60000|\n",
            "| Rahul|Engineering| 65000|          125000|\n",
            "|Naveen|Engineering| 70000|          195000|\n",
            "|Ananya|         HR| 52000|           52000|\n",
            "| Karan|         HR| 53000|          105000|\n",
            "|Fatima|  Marketing| 45000|           45000|\n",
            "|  Zoya|  Marketing| 48000|           93000|\n",
            "+------+-----------+------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Date Operations"
      ],
      "metadata": {
        "id": "YVHih3AOkk7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Add JoinDate with random dates (2020â€“2023)\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "def random_date(start_year=2020, end_year=2023):\n",
        "    start = datetime(start_year, 1, 1)\n",
        "    end = datetime(end_year, 12, 31)\n",
        "    delta = end - start\n",
        "    random_days = random.randint(0, delta.days)\n",
        "    return (start + timedelta(days=random_days)).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "random_date_udf = udf(random_date, StringType())\n",
        "df_dated = df_joined.withColumn(\"JoinDate\", random_date_udf())\n",
        "df_dated.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lr1xmhFRjai1",
        "outputId": "c905b39c-611f-4094-dfb2-2f0c12b76a74"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+------+----+------+----------+\n",
            "|  Name| Department|Salary|Year|Rating|  JoinDate|\n",
            "+------+-----------+------+----+------+----------+\n",
            "|Ananya|         HR| 52000|2023|   4.5|2020-05-02|\n",
            "|Fatima|  Marketing| 45000|2023|   3.9|2022-04-15|\n",
            "| Karan|         HR| 53000|2023|   4.1|2023-12-24|\n",
            "|Naveen|Engineering| 70000|2023|   4.7|2021-02-18|\n",
            "| Priya|Engineering| 60000|2023|   4.3|2021-09-11|\n",
            "| Rahul|Engineering| 65000|2023|   4.9|2023-08-09|\n",
            "|  Zoya|  Marketing| 48000|2023|   3.8|2022-05-07|\n",
            "+------+-----------+------+----+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Add YearsWithCompany\n",
        "from pyspark.sql.functions import current_date, datediff, col, round\n",
        "df_with_tenure = df_dated.withColumn(\"JoinDate\", col(\"JoinDate\").cast(\"date\"))\n",
        "df_with_tenure = df_with_tenure.withColumn(\"YearsWithCompany\", round(datediff(current_date(), col(\"JoinDate\")) / 365, 2))\n",
        "df_with_tenure.select(\"Name\", \"JoinDate\", \"YearsWithCompany\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruE48_5hjsyd",
        "outputId": "068400cd-c524-4733-d6d1-0d59679f0617"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+----------------+\n",
            "|  Name|  JoinDate|YearsWithCompany|\n",
            "+------+----------+----------------+\n",
            "|Ananya|2021-03-04|            4.27|\n",
            "|Fatima|2023-01-24|            2.38|\n",
            "| Karan|2022-02-08|            3.34|\n",
            "|Naveen|2022-04-19|            3.15|\n",
            "| Priya|2021-10-29|            3.62|\n",
            "| Rahul|2022-08-09|            2.84|\n",
            "|  Zoya|2020-12-10|             4.5|\n",
            "+------+----------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Writing to Files"
      ],
      "metadata": {
        "id": "xaVW59ifko1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  11. Write full employee DataFrame to CSV with headers\n",
        "df_emp.write.csv(\"employee_data.csv\", header=True)"
      ],
      "metadata": {
        "id": "m5taJtcGj0Ti"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Save the joined DataFrame to Parquet\n",
        "df_joined.write.parquet(\"joined_data.parquet\")"
      ],
      "metadata": {
        "id": "SxxrHFgVj6e1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read back the saved CSV file:\n",
        "df_read_csv = spark.read.csv(\"employee_data.csv\", header=True, inferSchema=True)\n",
        "df_read_csv.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnyRmY4fkCtX",
        "outputId": "ef1c6a23-348f-4731-be0f-ce3234433668"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+------+\n",
            "|  Name| Department|Salary|\n",
            "+------+-----------+------+\n",
            "|  Zoya|  Marketing| 48000|\n",
            "| Karan|         HR| 53000|\n",
            "|Naveen|Engineering| 70000|\n",
            "|Fatima|  Marketing| 45000|\n",
            "|Ananya|         HR| 52000|\n",
            "| Rahul|Engineering| 65000|\n",
            "| Priya|Engineering| 60000|\n",
            "+------+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read back the saved Parquet file:\n",
        "df_read_parquet = spark.read.parquet(\"joined_data.parquet\")\n",
        "df_read_parquet.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbeLqE6YkMXh",
        "outputId": "1dff5aff-e744-47aa-efd7-8342606243e5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+------+----+------+\n",
            "|  Name| Department|Salary|Year|Rating|\n",
            "+------+-----------+------+----+------+\n",
            "|Ananya|         HR| 52000|2023|   4.5|\n",
            "|Fatima|  Marketing| 45000|2023|   3.9|\n",
            "| Karan|         HR| 53000|2023|   4.1|\n",
            "|Naveen|Engineering| 70000|2023|   4.7|\n",
            "| Priya|Engineering| 60000|2023|   4.3|\n",
            "| Rahul|Engineering| 65000|2023|   4.9|\n",
            "|  Zoya|  Marketing| 48000|2023|   3.8|\n",
            "+------+-----------+------+----+------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}